"""
Benchmark script to demonstrate embedding model caching benefits.
Creates multiple clients and shows cache reuse.
"""
import time
import os
import shutil
from memlayer.wrappers.openai import OpenAI

# Configuration
USER_1 = "user_alice"
USER_2 = "user_bob"
USER_3 = "user_charlie"

STORAGE_1 = "./cache_test_1"
STORAGE_2 = "./cache_test_2"
STORAGE_3 = "./cache_test_3"

def cleanup_all():
    """Remove all test storage."""
    for path in [STORAGE_1, STORAGE_2, STORAGE_3]:
        if os.path.exists(path):
            try:
                shutil.rmtree(path)
            except:
                pass

def test_without_cache():
    """
    Simulate the old behavior: Each client loads its own model.
    (This is just for reference - we can't actually disable the cache now)
    """
    print("\n" + "="*70)
    print("SCENARIO 1: Without Caching (Reference)")
    print("="*70)
    print("\nIn the old implementation, each client would initialize")
    print("its own embedding model, taking ~11 seconds per client.")
    print()
    
    # Hypothetical times based on profiling
    print("Client 1 initialization: ~11 seconds (load model)")
    print("Client 2 initialization: ~11 seconds (load model again)")
    print("Client 3 initialization: ~11 seconds (load model again)")
    print("-" * 70)
    print("Total time: ~33 seconds")
    
    return 33.0  # Estimated based on profiling

def test_with_cache():
    """
    Test the new behavior: Clients reuse cached models.
    """
    cleanup_all()
    
    print("\n" + "="*70)
    print("SCENARIO 2: With Caching (Current Implementation)")
    print("="*70)
    print("\nNow testing with actual model caching...")
    print()
    
    times = []
    
    # Client 1: Cold start
    print("Creating Client 1 (Alice)...")
    start = time.time()
    client1 = OpenAI(
        storage_path=STORAGE_1,
        user_id=USER_1
    )
    # Trigger lazy loading by calling chat
    client1.chat(messages=[{"role": "user", "content": "Hello, I'm Alice!"}])
    time1 = time.time() - start
    times.append(time1)
    print(f"   âœ… Client 1 initialized in {time1:.3f}s (fresh model load)")
    print(f"      - Embedding model loaded from disk")
    time.sleep(1)
    
    # Client 2: Should reuse cache
    print("\nCreating Client 2 (Bob)...")
    start = time.time()
    client2 = OpenAI(
        storage_path=STORAGE_2,
        user_id=USER_2
    )
    # Trigger lazy loading
    client2.chat(messages=[{"role": "user", "content": "Hello, I'm Bob!"}])
    time2 = time.time() - start
    times.append(time2)
    print(f"   âœ… Client 2 initialized in {time2:.3f}s")
    print(f"      - Embedding model REUSED from cache!")
    saved2 = time1 - time2
    print(f"      - Saved ~{saved2:.1f}s compared to Client 1")
    time.sleep(1)
    
    # Client 3: Should also reuse cache
    print("\nCreating Client 3 (Charlie)...")
    start = time.time()
    client3 = OpenAI(
        storage_path=STORAGE_3,
        user_id=USER_3
    )
    # Trigger lazy loading
    client3.chat(messages=[{"role": "user", "content": "Hello, I'm Charlie!"}])
    time3 = time.time() - start
    times.append(time3)
    print(f"   âœ… Client 3 initialized in {time3:.3f}s")
    print(f"      - Embedding model REUSED from cache!")
    saved3 = time1 - time3
    print(f"      - Saved ~{saved3:.1f}s compared to Client 1")
    
    total_time = sum(times)
    print("\n" + "-"*70)
    print(f"Total time for 3 clients: {total_time:.3f}s")
    
    # Cleanup
    client1.close()
    client2.close()
    client3.close()
    time.sleep(0.5)
    cleanup_all()
    
    return total_time

def main():
    """Run cache benchmark and show improvement."""
    print("="*70)
    print("EMBEDDING MODEL CACHE BENCHMARK")
    print("="*70)
    print("\nThis benchmark demonstrates the benefits of model caching")
    print("when creating multiple Memlayer clients in the same process.")
    print()
    
    # Check API key
    if not os.getenv("OPENAI_API_KEY"):
        print("ERROR: Please set your OPENAI_API_KEY environment variable.")
        return
    
    # Reference scenario (estimated)
    time_without_cache = test_without_cache()
    
    # Actual test with caching
    time_with_cache = test_with_cache()
    
    # Results
    print("\n" + "="*70)
    print("RESULTS")
    print("="*70)
    
    improvement = time_without_cache - time_with_cache
    improvement_pct = (improvement / time_without_cache) * 100
    
    print(f"\nðŸ“Š Time without caching (old):  ~{time_without_cache:.1f}s (estimated)")
    print(f"ðŸ“Š Time with caching (new):      {time_with_cache:.1f}s (actual)")
    print(f"âš¡ Improvement:                   ~{improvement:.1f}s ({improvement_pct:.1f}% faster)")
    
    print("\n" + "="*70)
    print("ANALYSIS")
    print("="*70)
    print("\nâœ… The first client loads the embedding model (~11-12s)")
    print("âœ… Subsequent clients reuse the cached model (saves ~11s each)")
    print("âœ… This is especially beneficial for:")
    print("   - Multi-user applications")
    print("   - Services with multiple Memlayer instances")
    print("   - Testing scenarios with many clients")
    
    print("\nðŸ’¡ Note: The cache persists for the lifetime of the Python process.")
    print("   When the process exits, the cache is cleared.")
    
    print("\n" + "="*70)

if __name__ == "__main__":
    main()
