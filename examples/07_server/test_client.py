"""
Test client for Memlayer Server using OpenAI SDK.

This demonstrates how to use any OpenAI-compatible client with Memlayer proxy,
getting automatic memory capabilities without changing your code.

Prerequisites:
1. Start llama-server: ./llama-server -m model.gguf --port 8080 --chat-template llama3
2. Start Memlayer proxy: python3.12 -m memlayer.server
3. Run this script: python3.12 examples/07_server/test_client.py
"""

from openai import OpenAI
import time

print("=" * 70)
print("Memlayer Server - OpenAI SDK Test Client")
print("=" * 70)

# Initialize OpenAI client pointing to Memlayer proxy
client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="not-needed",  # Memlayer doesn't require authentication
)

print("\nâœ… Connected to Memlayer proxy at http://localhost:8000")
print("\n" + "=" * 70)
print("ğŸ“ Phase 1: Teaching the LLM about yourself")
print("=" * 70)

# First conversation - storing information
print("\nğŸ’¬ Message 1: Introducing yourself...")
response = client.chat.completions.create(
    model="qwen2.5:7b",  # Your llama-server model name
    messages=[
        {"role": "user", "content": "Hello! My name is Jordan and I'm a software architect."}
    ],
    temperature=0.7,
)
print(f"Assistant: {response.choices[0].message.content}")

print("\nğŸ’¬ Message 2: Sharing your work...")
response = client.chat.completions.create(
    model="qwen2.5:7b",
    messages=[
        {"role": "user", "content": "I work on designing microservices architectures using Kubernetes and gRPC."}
    ],
    temperature=0.7,
)
print(f"Assistant: {response.choices[0].message.content}")

print("\nğŸ’¬ Message 3: Mentioning your tech stack...")
response = client.chat.completions.create(
    model="qwen2.5:7b",
    messages=[
        {"role": "user", "content": "My favorite technologies are Go, Python, and PostgreSQL."}
    ],
    temperature=0.7,
)
print(f"Assistant: {response.choices[0].message.content}")

# Wait for background consolidation
print("\nâ³ Waiting for memory consolidation (3 seconds)...")
time.sleep(3)

print("\n" + "=" * 70)
print("ğŸ” Phase 2: Testing memory recall")
print("=" * 70)

print("\nğŸ’¬ Query 1: Asking about your name...")
response = client.chat.completions.create(
    model="qwen2.5:7b",
    messages=[
        {"role": "user", "content": "What's my name?"}
    ],
    temperature=0.7,
)
print(f"Assistant: {response.choices[0].message.content}")

print("\nğŸ’¬ Query 2: Asking about your profession...")
response = client.chat.completions.create(
    model="qwen2.5:7b",
    messages=[
        {"role": "user", "content": "What do I do for work?"}
    ],
    temperature=0.7,
)
print(f"Assistant: {response.choices[0].message.content}")

print("\nğŸ’¬ Query 3: Asking about your tech stack...")
response = client.chat.completions.create(
    model="qwen2.5:7b",
    messages=[
        {"role": "user", "content": "What technologies do I like to use?"}
    ],
    temperature=0.7,
)
print(f"Assistant: {response.choices[0].message.content}")

print("\n" + "=" * 70)
print("ğŸš€ Phase 3: Testing streaming")
print("=" * 70)

print("\nğŸ’¬ Streaming query: Tell me about myself...")
print("Assistant: ", end="", flush=True)

stream = client.chat.completions.create(
    model="qwen2.5:7b",
    messages=[
        {"role": "user", "content": "Tell me everything you remember about me."}
    ],
    temperature=0.7,
    stream=True,
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)

print("\n")

print("\n" + "=" * 70)
print("âœ… Test Complete!")
print("=" * 70)
print("\nğŸ’¡ Key Takeaways:")
print("  âœ“ Drop-in OpenAI API replacement")
print("  âœ“ Automatic memory storage and retrieval")
print("  âœ“ 100% offline operation (no API keys needed)")
print("  âœ“ Works with any OpenAI-compatible client")
print("  âœ“ Streaming support")
print("\nğŸ“Š Behind the scenes:")
print("  â€¢ Memories stored in ./memlayer_server_data/default_user/")
print("  â€¢ Local sentence-transformers for embeddings")
print("  â€¢ ChromaDB for vector search")
print("  â€¢ NetworkX for knowledge graph")
print("  â€¢ llama-server for LLM inference")
print("\nğŸ”§ Next steps:")
print("  â€¢ Try multi-user support with X-User-ID header")
print("  â€¢ Explore tool calling (function calling)")
print("  â€¢ Test with different models")
print("  â€¢ Integrate into your existing OpenAI workflows")
